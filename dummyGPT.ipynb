{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMbfprniruBnbiMeBJVzKzf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaykef/dummyGPT/blob/main/dummyGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the GPT model\n",
        "class dummyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.transformer = nn.Transformer(hidden_size, num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src)\n",
        "        tgt = self.embedding(tgt)\n",
        "        out = self.transformer(src, tgt)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Define the dataset\n",
        "data = [\"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"The quick brown fox jumps over the lazy dog again.\",\n",
        "        \"The quick brown fox jumps over the lazy dog one more time.\",\n",
        "        \"The quick brown fox jumps over the lazy dog once more.\",]\n",
        "\n",
        "# Convert text to numerical data\n",
        "word_to_index = {}\n",
        "index_to_word = {}\n",
        "for sentence in data:\n",
        "    for word in sentence.split():\n",
        "        if word not in word_to_index:\n",
        "            index = len(word_to_index)\n",
        "            word_to_index[word] = index\n",
        "            index_to_word[index] = word\n",
        "\n",
        "X = torch.tensor([word_to_index[word] for sentence in data for word in sentence.split()[:-1]])\n",
        "Y = torch.tensor([word_to_index[word] for sentence in data for word in sentence.split()[1:]])\n",
        "\n",
        "# Define model parameters\n",
        "vocab_size = len(word_to_index)\n",
        "embedding_size = 128\n",
        "hidden_size = 128\n",
        "num_layers = 1\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "model = dummyGPT(vocab_size, embedding_size, hidden_size, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X, Y)\n",
        "    loss = criterion(outputs.view(-1, vocab_size), Y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"Epoch {}: loss={}\".format(epoch, loss.item()))\n",
        "\n",
        "# Generate text given a prompt\n",
        "prompt = \"The quick brown\"\n",
        "prompt_tensor = torch.tensor([word_to_index[word] for word in prompt.split()])\n",
        "output = model(prompt_tensor.unsqueeze(0), prompt_tensor.unsqueeze(0))\n",
        "next_word_index = torch.argmax(output[-1]).item()\n",
        "next_word = index_to_word.get(next_word_index, \"<unk>\")\n",
        "print(prompt + \" \" + next_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oVFk-aTIEz5",
        "outputId": "ebef2090-30bf-404a-d4da-059142401422"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss=2.8905832767486572\n",
            "Epoch 1: loss=2.654731035232544\n",
            "Epoch 2: loss=2.8623907566070557\n",
            "Epoch 3: loss=2.5315873622894287\n",
            "Epoch 4: loss=2.570763349533081\n",
            "Epoch 5: loss=2.5244534015655518\n",
            "Epoch 6: loss=2.300091505050659\n",
            "Epoch 7: loss=1.7719885110855103\n",
            "Epoch 8: loss=2.319387435913086\n",
            "Epoch 9: loss=1.6609715223312378\n",
            "Epoch 10: loss=1.0500911474227905\n",
            "Epoch 11: loss=0.8445582389831543\n",
            "Epoch 12: loss=1.1472954750061035\n",
            "Epoch 13: loss=0.6941309571266174\n",
            "Epoch 14: loss=0.37709322571754456\n",
            "Epoch 15: loss=0.3787277340888977\n",
            "Epoch 16: loss=0.2671932280063629\n",
            "Epoch 17: loss=0.23273031413555145\n",
            "Epoch 18: loss=0.1989457905292511\n",
            "Epoch 19: loss=0.20245017111301422\n",
            "Epoch 20: loss=0.1466311514377594\n",
            "Epoch 21: loss=0.10781830549240112\n",
            "Epoch 22: loss=0.10073105245828629\n",
            "Epoch 23: loss=0.08664675801992416\n",
            "Epoch 24: loss=0.0751519650220871\n",
            "Epoch 25: loss=0.07154914736747742\n",
            "Epoch 26: loss=0.06905681639909744\n",
            "Epoch 27: loss=0.050838619470596313\n",
            "Epoch 28: loss=0.04459851607680321\n",
            "Epoch 29: loss=0.04519210010766983\n",
            "Epoch 30: loss=0.04111509770154953\n",
            "Epoch 31: loss=0.039196696132421494\n",
            "Epoch 32: loss=0.03012233041226864\n",
            "Epoch 33: loss=0.029613623395562172\n",
            "Epoch 34: loss=0.029663823544979095\n",
            "Epoch 35: loss=0.02809654362499714\n",
            "Epoch 36: loss=0.027819538488984108\n",
            "Epoch 37: loss=0.02446077950298786\n",
            "Epoch 38: loss=0.022935008630156517\n",
            "Epoch 39: loss=0.02250593528151512\n",
            "Epoch 40: loss=0.02069929800927639\n",
            "Epoch 41: loss=0.01896514743566513\n",
            "Epoch 42: loss=0.01921236515045166\n",
            "Epoch 43: loss=0.018640344962477684\n",
            "Epoch 44: loss=0.01852719858288765\n",
            "Epoch 45: loss=0.01625875197350979\n",
            "Epoch 46: loss=0.01595165953040123\n",
            "Epoch 47: loss=0.015420033596456051\n",
            "Epoch 48: loss=0.014744346030056477\n",
            "Epoch 49: loss=0.01404497679322958\n",
            "Epoch 50: loss=0.013443170115351677\n",
            "Epoch 51: loss=0.014328030869364738\n",
            "Epoch 52: loss=0.013305854983627796\n",
            "Epoch 53: loss=0.013060281984508038\n",
            "Epoch 54: loss=0.01226772740483284\n",
            "Epoch 55: loss=0.01225112285465002\n",
            "Epoch 56: loss=0.012315886095166206\n",
            "Epoch 57: loss=0.011685569770634174\n",
            "Epoch 58: loss=0.011540967971086502\n",
            "Epoch 59: loss=0.011408217251300812\n",
            "Epoch 60: loss=0.011067303828895092\n",
            "Epoch 61: loss=0.01085912249982357\n",
            "Epoch 62: loss=0.010828766040503979\n",
            "Epoch 63: loss=0.010515544563531876\n",
            "Epoch 64: loss=0.010091549716889858\n",
            "Epoch 65: loss=0.010440767742693424\n",
            "Epoch 66: loss=0.010225038975477219\n",
            "Epoch 67: loss=0.00965891219675541\n",
            "Epoch 68: loss=0.009693551808595657\n",
            "Epoch 69: loss=0.009966199286282063\n",
            "Epoch 70: loss=0.009690294042229652\n",
            "Epoch 71: loss=0.009246483445167542\n",
            "Epoch 72: loss=0.009127841331064701\n",
            "Epoch 73: loss=0.00891454704105854\n",
            "Epoch 74: loss=0.00891423411667347\n",
            "Epoch 75: loss=0.008921398781239986\n",
            "Epoch 76: loss=0.008642854169011116\n",
            "Epoch 77: loss=0.008596271276473999\n",
            "Epoch 78: loss=0.008642447181046009\n",
            "Epoch 79: loss=0.008616624400019646\n",
            "Epoch 80: loss=0.008474980480968952\n",
            "Epoch 81: loss=0.00841040164232254\n",
            "Epoch 82: loss=0.008122448809444904\n",
            "Epoch 83: loss=0.008180961012840271\n",
            "Epoch 84: loss=0.007750487420707941\n",
            "Epoch 85: loss=0.0076064239256083965\n",
            "Epoch 86: loss=0.0076358867809176445\n",
            "Epoch 87: loss=0.007826963439583778\n",
            "Epoch 88: loss=0.0076462142169475555\n",
            "Epoch 89: loss=0.007489796727895737\n",
            "Epoch 90: loss=0.007598811760544777\n",
            "Epoch 91: loss=0.0073356423527002335\n",
            "Epoch 92: loss=0.007109667174518108\n",
            "Epoch 93: loss=0.007174996193498373\n",
            "Epoch 94: loss=0.007325985003262758\n",
            "Epoch 95: loss=0.007062483113259077\n",
            "Epoch 96: loss=0.007161957677453756\n",
            "Epoch 97: loss=0.006932321935892105\n",
            "Epoch 98: loss=0.0068218582309782505\n",
            "Epoch 99: loss=0.006861866917461157\n",
            "The quick brown once\n"
          ]
        }
      ]
    }
  ]
}